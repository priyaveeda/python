{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18240145",
   "metadata": {},
   "source": [
    "# Extranet Molecule website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc764d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priya.C2657\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'extranet.edqm.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from re import sub\n",
    "import ssl\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import xml.etree.ElementTree as ET\n",
    "import certifi\n",
    "import urllib\n",
    "g = []\n",
    "wa1='https://extranet.edqm.eu/4DLink1/4DCGI/Query_CEP?vSelectName=1&Case_TSE=none&vContains=1&vContainsDate=1&vtsubName=Tiotropium+bromide&vtsubDateBegin=&vtsubDateBtwBegin=&vtsubDateBtwEnd=&SWTP=1&OK=Search'\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "for i in range(1, 2):\n",
    "    wa = wa1 + str(i)\n",
    "    #print(wa)\n",
    "    x = urllib.request.urlopen(wa).read()\n",
    "    response = requests.get(wa,verify=False)\n",
    "    rt = response.text\n",
    "    soup = BeautifulSoup(rt, \"html.parser\")\n",
    "    tables = soup.find_all('table')[2].find_all('tr')\n",
    "    #print(tables)\n",
    "    for table in tables:\n",
    "        h=[]\n",
    "        for row in table.find_all('td'):\n",
    "            try:\n",
    "                dat = row.text\n",
    "                #print(dat)\n",
    "            except:\n",
    "                dat = \"\"\n",
    "            h.append(dat)\n",
    "        g.append(h[0:8])\n",
    "df = pd.DataFrame(g, columns=['Substance Number', 'Substance ', 'Certificate Holder', 'Certificate Number','Issue Date','Status','End date','Type'])\n",
    "df1 = df.transpose()\n",
    "df.to_excel('C:/Python/Python/excel/data/Tiotropium bromide.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85050c0c",
   "metadata": {},
   "source": [
    "# ClinicalTrials Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edbf37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "import ssl\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import xml.etree.ElementTree as ET\n",
    "import certifi\n",
    "import urllib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = []\n",
    "\n",
    "#wa1 = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query=&page='\n",
    "wa1 = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query=&phase=phase-two&page='\n",
    "\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#ssl.SSLContext.verify_mode = property(lambda self: ssl.CERT_NONE, lambda self, newval: None)\n",
    "\n",
    "for i in range(1,951):\n",
    "    wa = wa1 + str(i) \n",
    "    #print(wa)\n",
    "    x = urllib.request.urlopen(wa).read()\n",
    "#     print(x)\n",
    "    response = requests.get(wa,verify=False)\n",
    "    rt = response.text\n",
    "    soup = BeautifulSoup(rt, \"html.parser\")\n",
    "    tables = soup.find_all('table', class_=\"result\")\n",
    "    #print(tables)\n",
    "    for table in tables:\n",
    "        h=[]\n",
    "        for row in table.find_all('tr'):\n",
    "            for column in row.find_all('td'):\n",
    "                try:\n",
    "                    dat = column.text.strip().split(':')[1]\n",
    "                except:\n",
    "                    dat = \"\"\n",
    "                h.append(dat)\n",
    "        g.append(h[0:6])\n",
    "    \n",
    "#print(g)\n",
    "\n",
    "df = pd.DataFrame(g, columns=['EudraCT Number', 'Protocol Number', 'Start Date', 'Sponsor Name','Title','Medical Condition'])\n",
    "df1 = df.transpose()\n",
    "# print(\"Print to excel\")\n",
    "df.to_excel('clinicaltrials.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6455a",
   "metadata": {},
   "source": [
    "# CTRI Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "wa = 'http://www.ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=50774'\n",
    "#print(wa)\n",
    "\n",
    "class HTMLTableParser:\n",
    "       \n",
    "    def parse_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        return [(table['id'],self.parse_html_table(table))\\\n",
    "                for table in soup.find_all('table')]  \n",
    "\n",
    "    def parse_subtable_h(self, table):\n",
    "        sub_dir = []\n",
    "        first_row = 1\n",
    "        for row in table.find_all('tr'):\n",
    "            ostr = \"\"\n",
    "            for column in row.find_all('td'):\n",
    "                sub_dir.append(column.text)\n",
    "        return sub_dir\n",
    "\n",
    "\n",
    "    def parse_html_table(self, table):\n",
    "        \n",
    "        df = {}\n",
    "        row_index = 0\n",
    "        edit_vals = {10:5, 20:3}\n",
    "        pharma_trial = 0\n",
    "        for row in table.find_all('tr', recursive=False):\n",
    "            columns = row.find_all('td')\n",
    "            c0 = columns[0].text.replace('\\n','').strip()\n",
    "            subtable_found = 0\n",
    "            try:\n",
    "                subtable = columns[1].find_all('table')[0]\n",
    "                subtable_found = 1\n",
    "                #print('Subtable found', subtable)\n",
    "            except:\n",
    "                subtable_found = 0\n",
    "                pass\n",
    "            if (subtable_found == 1):\n",
    "                #print(\"Subtable found\")\n",
    "                sub_text = self.parse_subtable_h(subtable)\n",
    "                \n",
    "                if row_index in edit_vals.keys():\n",
    "                    c1 = sub_text[edit_vals[row_index]]\n",
    "                elif row_index == 14:\n",
    "                    c1 = sub_text[1]\n",
    "                    df['Sponsor Address'] = sub_text[3]\n",
    "                    df['Sponsor Type'] = sub_text[5]\n",
    "                    #print(sub_text[5][:10])\n",
    "                    if sub_text[5][:14] == \"Pharmaceutical\":\n",
    "                        print(\"pharma trial\")\n",
    "                        pharma_trial = 1\n",
    "                elif row_index == 21:\n",
    "                    if pharma_trial == 1:\n",
    "                        try:\n",
    "                            cindx = sub_text.index('Intervention\\xa0')\n",
    "                            c1 = sub_text[cindx + 1]\n",
    "                        except:\n",
    "                            c1 = \"Could not parse details\"\n",
    "                    else:\n",
    "                        c1 = \"\"\n",
    "                else:\n",
    "                    c1 = \", \".join(sub_text)\n",
    "            else:\n",
    "                c1 = columns[1].text    \n",
    "                if row_index == 0:\n",
    "                    c1 = c1[:20]\n",
    "            df[c0] = c1.strip('\\n')\n",
    "            row_index += 1\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"C:/Python/Python/ctri.mht\") as f:\n",
    "    rt1 = f.read()\n",
    "\n",
    "#print(rt)\n",
    "print(type(rt1))\n",
    "\n",
    "import re\n",
    "g=[m.start() for m in re.finditer(\"value=3D\", rt1)]\n",
    "#print(g)\n",
    "print(len(g))\n",
    "wa1 = \"http://www.ctri.nic.in/Clinicaltrials/pmaindet2.php?trialid=\"\n",
    "df = pd.DataFrame()\n",
    "for x in g:\n",
    "    trialID = rt1[x+9:x+14]\n",
    "    \n",
    "    wa = wa1 + str(trialID)\n",
    "    print(wa)\n",
    "    ctri_study = 0\n",
    "    try:\n",
    "        response = requests.get(wa)\n",
    "        rt = response.text\n",
    "        ctri_study = 1\n",
    "    except:\n",
    "        print('no response')\n",
    "    if (ctri_study):\n",
    "        soup = BeautifulSoup(rt, 'html.parser')\n",
    "        table = soup.find_all('table')[2]\n",
    "        #print(table)\n",
    "        hp = HTMLTableParser()\n",
    "        df1 = hp.parse_html_table(table)\n",
    "        #print(df1)\n",
    "        df=pd.concat([df, pd.DataFrame([df1])])\n",
    "        df['link'] = wa\n",
    "        \"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(rt, 'html.parser')\n",
    "            table = soup.find_all('table')[2]\n",
    "            print(table)\n",
    "            hp = HTMLTableParser()\n",
    "            df1 = hp.parse_html_table(table)\n",
    "            print(df1)\n",
    "            df=pd.concat([df, pd.DataFrame([df1])])\n",
    "            df['link'] = wa\n",
    "        except:\n",
    "            print(\"Could nto parse: \" + wa)\n",
    "            pass\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "print(\"Print to excel\")\n",
    "\n",
    "df.to_excel('test.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
